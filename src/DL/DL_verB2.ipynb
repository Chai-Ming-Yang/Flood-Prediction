{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UGq7APZYhHGP"
      },
      "outputs": [],
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (8,6)\n",
        "mpl.rcParams['axes.grid'] = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcG-tvu5zA0y"
      },
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "H-MzuWewr8tR",
        "outputId": "bf0a8042-2b33-46f9-b6fc-64b915dff9e8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SUBDIVISIONS</th>\n",
              "      <th>YEAR</th>\n",
              "      <th>JAN</th>\n",
              "      <th>FEB</th>\n",
              "      <th>MAR</th>\n",
              "      <th>APR</th>\n",
              "      <th>MAY</th>\n",
              "      <th>JUN</th>\n",
              "      <th>JUL</th>\n",
              "      <th>AUG</th>\n",
              "      <th>SEP</th>\n",
              "      <th>OCT</th>\n",
              "      <th>NOV</th>\n",
              "      <th>DEC</th>\n",
              "      <th>ANNUAL</th>\n",
              "      <th>FLOOD</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ANDAMAN &amp; NICOBAR ISLANDS</td>\n",
              "      <td>2019</td>\n",
              "      <td>173.8</td>\n",
              "      <td>5.8</td>\n",
              "      <td>15.8</td>\n",
              "      <td>35.3</td>\n",
              "      <td>230.9</td>\n",
              "      <td>662.2</td>\n",
              "      <td>212.0</td>\n",
              "      <td>860.4</td>\n",
              "      <td>596.8</td>\n",
              "      <td>136.6</td>\n",
              "      <td>131.9</td>\n",
              "      <td>24.5</td>\n",
              "      <td>3086.0</td>\n",
              "      <td>YES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ANDAMAN &amp; NICOBAR ISLANDS</td>\n",
              "      <td>2021</td>\n",
              "      <td>42.7</td>\n",
              "      <td>48.8</td>\n",
              "      <td>38.3</td>\n",
              "      <td>150.2</td>\n",
              "      <td>414.1</td>\n",
              "      <td>315.9</td>\n",
              "      <td>535.3</td>\n",
              "      <td>506.5</td>\n",
              "      <td>667.3</td>\n",
              "      <td>413.1</td>\n",
              "      <td>265.7</td>\n",
              "      <td>95.5</td>\n",
              "      <td>3493.4</td>\n",
              "      <td>YES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ANDAMAN &amp; NICOBAR ISLANDS</td>\n",
              "      <td>2016</td>\n",
              "      <td>72.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>5.4</td>\n",
              "      <td>2.4</td>\n",
              "      <td>191.1</td>\n",
              "      <td>429.4</td>\n",
              "      <td>301.2</td>\n",
              "      <td>227.7</td>\n",
              "      <td>604.3</td>\n",
              "      <td>287.2</td>\n",
              "      <td>181.7</td>\n",
              "      <td>533.7</td>\n",
              "      <td>2851.9</td>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ANDAMAN &amp; NICOBAR ISLANDS</td>\n",
              "      <td>2017</td>\n",
              "      <td>228.7</td>\n",
              "      <td>5.6</td>\n",
              "      <td>33.0</td>\n",
              "      <td>108.3</td>\n",
              "      <td>275.8</td>\n",
              "      <td>349.1</td>\n",
              "      <td>389.4</td>\n",
              "      <td>414.7</td>\n",
              "      <td>372.8</td>\n",
              "      <td>263.0</td>\n",
              "      <td>205.9</td>\n",
              "      <td>243.7</td>\n",
              "      <td>2890.0</td>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ANDAMAN &amp; NICOBAR ISLANDS</td>\n",
              "      <td>2018</td>\n",
              "      <td>167.3</td>\n",
              "      <td>36.2</td>\n",
              "      <td>21.5</td>\n",
              "      <td>90.0</td>\n",
              "      <td>372.5</td>\n",
              "      <td>518.4</td>\n",
              "      <td>239.1</td>\n",
              "      <td>415.7</td>\n",
              "      <td>395.9</td>\n",
              "      <td>298.9</td>\n",
              "      <td>239.6</td>\n",
              "      <td>318.4</td>\n",
              "      <td>3113.5</td>\n",
              "      <td>YES</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                SUBDIVISIONS  YEAR    JAN   FEB   MAR    APR    MAY    JUN  \\\n",
              "0  ANDAMAN & NICOBAR ISLANDS  2019  173.8   5.8  15.8   35.3  230.9  662.2   \n",
              "1  ANDAMAN & NICOBAR ISLANDS  2021   42.7  48.8  38.3  150.2  414.1  315.9   \n",
              "2  ANDAMAN & NICOBAR ISLANDS  2016   72.0  15.8   5.4    2.4  191.1  429.4   \n",
              "3  ANDAMAN & NICOBAR ISLANDS  2017  228.7   5.6  33.0  108.3  275.8  349.1   \n",
              "4  ANDAMAN & NICOBAR ISLANDS  2018  167.3  36.2  21.5   90.0  372.5  518.4   \n",
              "\n",
              "     JUL    AUG    SEP    OCT    NOV    DEC  ANNUAL FLOOD  \n",
              "0  212.0  860.4  596.8  136.6  131.9   24.5  3086.0   YES  \n",
              "1  535.3  506.5  667.3  413.1  265.7   95.5  3493.4   YES  \n",
              "2  301.2  227.7  604.3  287.2  181.7  533.7  2851.9    NO  \n",
              "3  389.4  414.7  372.8  263.0  205.9  243.7  2890.0    NO  \n",
              "4  239.1  415.7  395.9  298.9  239.6  318.4  3113.5   YES  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv('..\\..\\datasets\\B2_Monthly_Rainfall.csv')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrZSeKLQsL5p",
        "outputId": "c8b7bed7-9af6-430b-c1e8-ac52fb3e950a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14380\\112302960.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df['FLOOD'] = df['FLOOD'].replace({'YES': 1, 'NO': 0})\n"
          ]
        }
      ],
      "source": [
        "df = data.copy()\n",
        "df['FLOOD'] = df['FLOOD'].replace({'YES': 1, 'NO': 0})\n",
        "df['FLOOD'] = df['FLOOD'].fillna(0).astype(int)\n",
        "df.sort_values(by=['YEAR'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y48C_YEWsMXi",
        "outputId": "5cc418ca-a53c-434a-e183-f42d7cb6a565"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "df shape : (230, 16) \n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 230 entries, 2 to 174\n",
            "Data columns (total 16 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   SUBDIVISIONS  230 non-null    object \n",
            " 1   YEAR          230 non-null    int64  \n",
            " 2   JAN           230 non-null    float64\n",
            " 3   FEB           230 non-null    float64\n",
            " 4   MAR           230 non-null    float64\n",
            " 5   APR           230 non-null    float64\n",
            " 6   MAY           230 non-null    float64\n",
            " 7   JUN           230 non-null    float64\n",
            " 8   JUL           230 non-null    float64\n",
            " 9   AUG           230 non-null    float64\n",
            " 10  SEP           230 non-null    float64\n",
            " 11  OCT           230 non-null    float64\n",
            " 12  NOV           230 non-null    float64\n",
            " 13  DEC           230 non-null    float64\n",
            " 14  ANNUAL        230 non-null    float64\n",
            " 15  FLOOD         230 non-null    int64  \n",
            "dtypes: float64(13), int64(2), object(1)\n",
            "memory usage: 30.5+ KB\n"
          ]
        }
      ],
      "source": [
        "print(\"df shape :\", df.shape, \"\\n\")\n",
        "df.info()\n",
        "# df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-HeOZMcs_PN"
      },
      "source": [
        "# Data Splitting & Window Sliding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1jRPcRAtsMhz"
      },
      "outputs": [],
      "source": [
        "X = df.drop(columns=['YEAR', 'FLOOD', 'SUBDIVISIONS'])\n",
        "Y = df['FLOOD']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tKh1tJ-2a0d_"
      },
      "outputs": [],
      "source": [
        "\"\"\" SLIDING WINDOW \"\"\"\n",
        "WINDOW_SIZE = 5\n",
        "def create_dataset(X, Y):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - WINDOW_SIZE):\n",
        "        v = X[i:(i + WINDOW_SIZE)]\n",
        "        Xs.append(v)\n",
        "        ys.append(Y.iloc[i + WINDOW_SIZE])\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "X_seq, Y_seq = create_dataset(X, Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Y9tXkWOyuDed"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_seq, Y_seq, test_size=0.2, shuffle=False)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gctLZSLqsMrO"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# --- Reshape for scaling ---\n",
        "X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n",
        "X_val_reshaped = X_val.reshape(-1, X_val.shape[-1])\n",
        "X_test_reshaped = X_test.reshape(-1, X_test.shape[-1])\n",
        "\n",
        "# --- Initialize and fit scaler ---\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_reshaped)\n",
        "\n",
        "# --- Transform validation and test data ---\n",
        "X_val_scaled = scaler.transform(X_val_reshaped)\n",
        "X_test_scaled = scaler.transform(X_test_reshaped)\n",
        "\n",
        "# --- Reshape back to original 3D shape ---\n",
        "X_train_seq = X_train_scaled.reshape(X_train.shape)\n",
        "X_val_seq = X_val_scaled.reshape(X_val.shape)\n",
        "X_test_seq = X_test_scaled.reshape(X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9IhosYQsMwx",
        "outputId": "04e528a9-c5f3-46ad-d855-2dee52943033"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(None, 5, 13)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Input\n",
        "from keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "def build_lstm_model(learning_rate=0.0001):\n",
        "  model = Sequential()\n",
        "  model.add(Input(shape=(WINDOW_SIZE, X.shape[1])))\n",
        "  model.add(LSTM(64, return_sequences=True))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(LSTM(32, return_sequences=True))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(LSTM(16, return_sequences=False))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  optimizer = Adam(learning_rate=learning_rate, clipvalue=1.0)\n",
        "  loss = BinaryCrossentropy()\n",
        "  model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "model = build_lstm_model()\n",
        "lr_scheduler = ReduceLROnPlateau(\n",
        "  monitor='val_loss',\n",
        "  factor=0.5,\n",
        "  patience=5,\n",
        "  min_lr=1e-5,\n",
        "  verbose=1\n",
        ")\n",
        "model.input_shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8Sf__wtsNDf",
        "outputId": "de367442-4063-4c4e-f623-881985931252"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 297ms/step - accuracy: 0.5377 - loss: 0.6957 - val_accuracy: 0.9444 - val_loss: 0.6861 - learning_rate: 1.0000e-04\n",
            "Epoch 2/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.5238 - loss: 0.7239 - val_accuracy: 0.9444 - val_loss: 0.6842 - learning_rate: 1.0000e-04\n",
            "Epoch 3/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.6091 - loss: 0.6770 - val_accuracy: 0.9444 - val_loss: 0.6823 - learning_rate: 1.0000e-04\n",
            "Epoch 4/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.5353 - loss: 0.6878 - val_accuracy: 0.9444 - val_loss: 0.6803 - learning_rate: 1.0000e-04\n",
            "Epoch 5/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.5806 - loss: 0.6821 - val_accuracy: 0.9444 - val_loss: 0.6782 - learning_rate: 1.0000e-04\n",
            "Epoch 6/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.5408 - loss: 0.7016 - val_accuracy: 0.9444 - val_loss: 0.6765 - learning_rate: 1.0000e-04\n",
            "Epoch 7/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.5322 - loss: 0.6756 - val_accuracy: 0.9444 - val_loss: 0.6745 - learning_rate: 1.0000e-04\n",
            "Epoch 8/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.5826 - loss: 0.6799 - val_accuracy: 0.9444 - val_loss: 0.6724 - learning_rate: 1.0000e-04\n",
            "Epoch 9/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.4967 - loss: 0.7086 - val_accuracy: 0.9444 - val_loss: 0.6705 - learning_rate: 1.0000e-04\n",
            "Epoch 10/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5850 - loss: 0.6705 - val_accuracy: 0.9444 - val_loss: 0.6684 - learning_rate: 1.0000e-04\n",
            "Epoch 11/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.5481 - loss: 0.6973 - val_accuracy: 0.9444 - val_loss: 0.6664 - learning_rate: 1.0000e-04\n",
            "Epoch 12/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.5938 - loss: 0.6656 - val_accuracy: 0.9444 - val_loss: 0.6640 - learning_rate: 1.0000e-04\n",
            "Epoch 13/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.6309 - loss: 0.6555 - val_accuracy: 0.9444 - val_loss: 0.6619 - learning_rate: 1.0000e-04\n",
            "Epoch 14/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.6374 - loss: 0.6481 - val_accuracy: 0.9444 - val_loss: 0.6597 - learning_rate: 1.0000e-04\n",
            "Epoch 15/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.6472 - loss: 0.6421 - val_accuracy: 0.9444 - val_loss: 0.6576 - learning_rate: 1.0000e-04\n",
            "Epoch 16/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.6681 - loss: 0.6431 - val_accuracy: 0.9444 - val_loss: 0.6553 - learning_rate: 1.0000e-04\n",
            "Epoch 17/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.5938 - loss: 0.6292 - val_accuracy: 0.9444 - val_loss: 0.6531 - learning_rate: 1.0000e-04\n",
            "Epoch 18/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.6010 - loss: 0.6512 - val_accuracy: 0.9444 - val_loss: 0.6511 - learning_rate: 1.0000e-04\n",
            "Epoch 19/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6536 - loss: 0.6465 - val_accuracy: 0.9444 - val_loss: 0.6488 - learning_rate: 1.0000e-04\n",
            "Epoch 20/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.6397 - loss: 0.6282 - val_accuracy: 0.9444 - val_loss: 0.6466 - learning_rate: 1.0000e-04\n",
            "Epoch 21/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.6681 - loss: 0.6245 - val_accuracy: 0.9444 - val_loss: 0.6443 - learning_rate: 1.0000e-04\n",
            "Epoch 22/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6463 - loss: 0.6262 - val_accuracy: 0.9444 - val_loss: 0.6423 - learning_rate: 1.0000e-04\n",
            "Epoch 23/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.6676 - loss: 0.6152 - val_accuracy: 0.9444 - val_loss: 0.6401 - learning_rate: 1.0000e-04\n",
            "Epoch 24/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.6569 - loss: 0.6138 - val_accuracy: 0.9444 - val_loss: 0.6379 - learning_rate: 1.0000e-04\n",
            "Epoch 25/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.7049 - loss: 0.6104 - val_accuracy: 0.9444 - val_loss: 0.6357 - learning_rate: 1.0000e-04\n",
            "Epoch 26/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6948 - loss: 0.6245 - val_accuracy: 0.9444 - val_loss: 0.6332 - learning_rate: 1.0000e-04\n",
            "Epoch 27/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.6743 - loss: 0.5999 - val_accuracy: 0.9444 - val_loss: 0.6307 - learning_rate: 1.0000e-04\n",
            "Epoch 28/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7535 - loss: 0.5882 - val_accuracy: 0.9444 - val_loss: 0.6283 - learning_rate: 1.0000e-04\n",
            "Epoch 29/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7031 - loss: 0.6182 - val_accuracy: 0.9444 - val_loss: 0.6257 - learning_rate: 1.0000e-04\n",
            "Epoch 30/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6545 - loss: 0.6103 - val_accuracy: 0.9444 - val_loss: 0.6231 - learning_rate: 1.0000e-04\n",
            "Epoch 31/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7400 - loss: 0.5806 - val_accuracy: 0.9444 - val_loss: 0.6206 - learning_rate: 1.0000e-04\n",
            "Epoch 32/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7015 - loss: 0.5981 - val_accuracy: 0.9444 - val_loss: 0.6183 - learning_rate: 1.0000e-04\n",
            "Epoch 33/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.7686 - loss: 0.5688 - val_accuracy: 0.9444 - val_loss: 0.6158 - learning_rate: 1.0000e-04\n",
            "Epoch 34/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.6998 - loss: 0.5994 - val_accuracy: 0.9444 - val_loss: 0.6134 - learning_rate: 1.0000e-04\n",
            "Epoch 35/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6956 - loss: 0.6040 - val_accuracy: 0.9444 - val_loss: 0.6109 - learning_rate: 1.0000e-04\n",
            "Epoch 36/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.7602 - loss: 0.5816 - val_accuracy: 0.9444 - val_loss: 0.6084 - learning_rate: 1.0000e-04\n",
            "Epoch 37/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7982 - loss: 0.5669 - val_accuracy: 0.9444 - val_loss: 0.6060 - learning_rate: 1.0000e-04\n",
            "Epoch 38/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7507 - loss: 0.5835 - val_accuracy: 0.9444 - val_loss: 0.6033 - learning_rate: 1.0000e-04\n",
            "Epoch 39/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.7901 - loss: 0.5606 - val_accuracy: 0.9444 - val_loss: 0.6010 - learning_rate: 1.0000e-04\n",
            "Epoch 40/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.7309 - loss: 0.5799 - val_accuracy: 0.9444 - val_loss: 0.5986 - learning_rate: 1.0000e-04\n",
            "Epoch 41/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.7558 - loss: 0.5654 - val_accuracy: 0.9444 - val_loss: 0.5963 - learning_rate: 1.0000e-04\n",
            "Epoch 42/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7507 - loss: 0.5476 - val_accuracy: 0.9444 - val_loss: 0.5938 - learning_rate: 1.0000e-04\n",
            "Epoch 43/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.7549 - loss: 0.5661 - val_accuracy: 0.9444 - val_loss: 0.5915 - learning_rate: 1.0000e-04\n",
            "Epoch 44/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7348 - loss: 0.5593 - val_accuracy: 0.9444 - val_loss: 0.5892 - learning_rate: 1.0000e-04\n",
            "Epoch 45/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7930 - loss: 0.5289 - val_accuracy: 0.9444 - val_loss: 0.5867 - learning_rate: 1.0000e-04\n",
            "Epoch 46/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8016 - loss: 0.5419 - val_accuracy: 0.9444 - val_loss: 0.5841 - learning_rate: 1.0000e-04\n",
            "Epoch 47/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7725 - loss: 0.5388 - val_accuracy: 0.9444 - val_loss: 0.5816 - learning_rate: 1.0000e-04\n",
            "Epoch 48/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8455 - loss: 0.5340 - val_accuracy: 0.9444 - val_loss: 0.5790 - learning_rate: 1.0000e-04\n",
            "Epoch 49/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8047 - loss: 0.5223 - val_accuracy: 0.9444 - val_loss: 0.5760 - learning_rate: 1.0000e-04\n",
            "Epoch 50/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7949 - loss: 0.5360 - val_accuracy: 0.9444 - val_loss: 0.5731 - learning_rate: 1.0000e-04\n",
            "Epoch 51/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7795 - loss: 0.5428 - val_accuracy: 0.9444 - val_loss: 0.5702 - learning_rate: 1.0000e-04\n",
            "Epoch 52/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8215 - loss: 0.5349 - val_accuracy: 0.9444 - val_loss: 0.5672 - learning_rate: 1.0000e-04\n",
            "Epoch 53/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8173 - loss: 0.5254 - val_accuracy: 0.9444 - val_loss: 0.5641 - learning_rate: 1.0000e-04\n",
            "Epoch 54/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8576 - loss: 0.5130 - val_accuracy: 0.9444 - val_loss: 0.5610 - learning_rate: 1.0000e-04\n",
            "Epoch 55/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8117 - loss: 0.5101 - val_accuracy: 0.9444 - val_loss: 0.5581 - learning_rate: 1.0000e-04\n",
            "Epoch 56/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.7821 - loss: 0.5259 - val_accuracy: 0.9444 - val_loss: 0.5550 - learning_rate: 1.0000e-04\n",
            "Epoch 57/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7806 - loss: 0.5369 - val_accuracy: 0.9444 - val_loss: 0.5519 - learning_rate: 1.0000e-04\n",
            "Epoch 58/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8210 - loss: 0.4949 - val_accuracy: 0.9444 - val_loss: 0.5489 - learning_rate: 1.0000e-04\n",
            "Epoch 59/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.7885 - loss: 0.5164 - val_accuracy: 0.9444 - val_loss: 0.5460 - learning_rate: 1.0000e-04\n",
            "Epoch 60/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8590 - loss: 0.5053 - val_accuracy: 0.9444 - val_loss: 0.5428 - learning_rate: 1.0000e-04\n",
            "Epoch 61/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8078 - loss: 0.5042 - val_accuracy: 0.9444 - val_loss: 0.5399 - learning_rate: 1.0000e-04\n",
            "Epoch 62/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8268 - loss: 0.5082 - val_accuracy: 0.9444 - val_loss: 0.5368 - learning_rate: 1.0000e-04\n",
            "Epoch 63/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8109 - loss: 0.5047 - val_accuracy: 0.9444 - val_loss: 0.5340 - learning_rate: 1.0000e-04\n",
            "Epoch 64/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.8455 - loss: 0.5096 - val_accuracy: 0.9444 - val_loss: 0.5309 - learning_rate: 1.0000e-04\n",
            "Epoch 65/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8525 - loss: 0.4961 - val_accuracy: 0.9444 - val_loss: 0.5281 - learning_rate: 1.0000e-04\n",
            "Epoch 66/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.7896 - loss: 0.5169 - val_accuracy: 0.9444 - val_loss: 0.5249 - learning_rate: 1.0000e-04\n",
            "Epoch 67/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8377 - loss: 0.4899 - val_accuracy: 0.9444 - val_loss: 0.5217 - learning_rate: 1.0000e-04\n",
            "Epoch 68/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8249 - loss: 0.4985 - val_accuracy: 0.9444 - val_loss: 0.5182 - learning_rate: 1.0000e-04\n",
            "Epoch 69/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7938 - loss: 0.5000 - val_accuracy: 0.9444 - val_loss: 0.5149 - learning_rate: 1.0000e-04\n",
            "Epoch 70/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8307 - loss: 0.4749 - val_accuracy: 0.9444 - val_loss: 0.5116 - learning_rate: 1.0000e-04\n",
            "Epoch 71/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8598 - loss: 0.4939 - val_accuracy: 0.9444 - val_loss: 0.5081 - learning_rate: 1.0000e-04\n",
            "Epoch 72/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8285 - loss: 0.4812 - val_accuracy: 0.9444 - val_loss: 0.5042 - learning_rate: 1.0000e-04\n",
            "Epoch 73/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8774 - loss: 0.4618 - val_accuracy: 0.9444 - val_loss: 0.5007 - learning_rate: 1.0000e-04\n",
            "Epoch 74/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8601 - loss: 0.4654 - val_accuracy: 0.9444 - val_loss: 0.4973 - learning_rate: 1.0000e-04\n",
            "Epoch 75/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8299 - loss: 0.4846 - val_accuracy: 0.9444 - val_loss: 0.4939 - learning_rate: 1.0000e-04\n",
            "Epoch 76/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.8377 - loss: 0.4631 - val_accuracy: 0.9444 - val_loss: 0.4906 - learning_rate: 1.0000e-04\n",
            "Epoch 77/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8576 - loss: 0.4615 - val_accuracy: 0.9444 - val_loss: 0.4875 - learning_rate: 1.0000e-04\n",
            "Epoch 78/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8159 - loss: 0.4898 - val_accuracy: 0.9444 - val_loss: 0.4843 - learning_rate: 1.0000e-04\n",
            "Epoch 79/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8307 - loss: 0.4792 - val_accuracy: 0.9444 - val_loss: 0.4811 - learning_rate: 1.0000e-04\n",
            "Epoch 80/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8979 - loss: 0.4466 - val_accuracy: 0.9444 - val_loss: 0.4778 - learning_rate: 1.0000e-04\n",
            "Epoch 81/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8260 - loss: 0.4656 - val_accuracy: 0.9444 - val_loss: 0.4748 - learning_rate: 1.0000e-04\n",
            "Epoch 82/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8556 - loss: 0.4620 - val_accuracy: 0.9444 - val_loss: 0.4717 - learning_rate: 1.0000e-04\n",
            "Epoch 83/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8458 - loss: 0.4686 - val_accuracy: 0.9444 - val_loss: 0.4683 - learning_rate: 1.0000e-04\n",
            "Epoch 84/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8615 - loss: 0.4625 - val_accuracy: 0.9444 - val_loss: 0.4652 - learning_rate: 1.0000e-04\n",
            "Epoch 85/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.8120 - loss: 0.4806 - val_accuracy: 0.9444 - val_loss: 0.4619 - learning_rate: 1.0000e-04\n",
            "Epoch 86/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8548 - loss: 0.4539 - val_accuracy: 0.9444 - val_loss: 0.4582 - learning_rate: 1.0000e-04\n",
            "Epoch 87/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8478 - loss: 0.4484 - val_accuracy: 0.9444 - val_loss: 0.4549 - learning_rate: 1.0000e-04\n",
            "Epoch 88/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.8816 - loss: 0.4523 - val_accuracy: 0.9444 - val_loss: 0.4518 - learning_rate: 1.0000e-04\n",
            "Epoch 89/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.8746 - loss: 0.4531 - val_accuracy: 0.9444 - val_loss: 0.4479 - learning_rate: 1.0000e-04\n",
            "Epoch 90/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8758 - loss: 0.4422 - val_accuracy: 0.9444 - val_loss: 0.4438 - learning_rate: 1.0000e-04\n",
            "Epoch 91/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8959 - loss: 0.4362 - val_accuracy: 0.9444 - val_loss: 0.4400 - learning_rate: 1.0000e-04\n",
            "Epoch 92/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.8579 - loss: 0.4462 - val_accuracy: 0.9444 - val_loss: 0.4363 - learning_rate: 1.0000e-04\n",
            "Epoch 93/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8886 - loss: 0.4343 - val_accuracy: 0.9444 - val_loss: 0.4328 - learning_rate: 1.0000e-04\n",
            "Epoch 94/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8847 - loss: 0.4327 - val_accuracy: 0.9444 - val_loss: 0.4294 - learning_rate: 1.0000e-04\n",
            "Epoch 95/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.8898 - loss: 0.4413 - val_accuracy: 0.9444 - val_loss: 0.4263 - learning_rate: 1.0000e-04\n",
            "Epoch 96/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8819 - loss: 0.4299 - val_accuracy: 0.9444 - val_loss: 0.4232 - learning_rate: 1.0000e-04\n",
            "Epoch 97/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.8828 - loss: 0.4324 - val_accuracy: 0.9444 - val_loss: 0.4201 - learning_rate: 1.0000e-04\n",
            "Epoch 98/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8886 - loss: 0.4238 - val_accuracy: 0.9444 - val_loss: 0.4169 - learning_rate: 1.0000e-04\n",
            "Epoch 99/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.8528 - loss: 0.4515 - val_accuracy: 0.9444 - val_loss: 0.4136 - learning_rate: 1.0000e-04\n",
            "Epoch 100/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.8859 - loss: 0.4170 - val_accuracy: 0.9444 - val_loss: 0.4103 - learning_rate: 1.0000e-04\n"
          ]
        }
      ],
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "history = model.fit(\n",
        "    X_train_seq, y_train,\n",
        "    validation_data=(X_val_seq, y_val),\n",
        "    epochs=100,\n",
        "    batch_size=64,\n",
        "    callbacks=[early_stopping, lr_scheduler]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "fKW4deeLv6aN",
        "outputId": "2c36b730-b415-4a97-f18d-4e87dbe42cfd"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def plot_history(history, key):\n",
        "  plt.figure()\n",
        "  plt.plot(history.history[key], label=key)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel(key)\n",
        "  plt.legend([key, 'val_'+key])\n",
        "  plt.title(f\"Training and Validation {key}\")\n",
        "  plt.show()\n",
        "\n",
        "plot_history(history, 'val_loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WB-JZXgiv6gG",
        "outputId": "988acb68-6a3d-40b5-869c-361f437a71a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 440ms/step\n",
            "Accuracy :  0.9111111111111111\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      1.00      0.95        41\n",
            "           1       0.00      0.00      0.00         4\n",
            "\n",
            "    accuracy                           0.91        45\n",
            "   macro avg       0.46      0.50      0.48        45\n",
            "weighted avg       0.83      0.91      0.87        45\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "y_pred = model.predict(X_test_seq)\n",
        "y_pred = (y_pred >= 0.5).astype(int)\n",
        "\n",
        "print(\"Accuracy : \", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "ws0FLP6g3GkU",
        "outputId": "10e0ddf2-92e8-42f2-b5af-5a5c91f1ba1a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Value</th>\n",
              "      <th>y_pred_count</th>\n",
              "      <th>y_test_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Value  y_pred_count  y_test_count\n",
              "0      0          45.0            41\n",
              "1      1           NaN             4"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Flatten the arrays before creating Series\n",
        "y_pred_flat = y_pred.ravel()  # or use y_pred.flatten()\n",
        "y_test_flat = y_test.ravel()  # or use y_test.flatten()\n",
        "\n",
        "# Create DataFrames and get value counts in one step\n",
        "y_pred_counts = pd.Series(y_pred_flat).value_counts().reset_index(name='y_pred_count')\n",
        "y_test_counts = pd.Series(y_test_flat).value_counts().reset_index(name='y_test_count')\n",
        "\n",
        "# Rename 'index' to 'Value' for both DataFrames before merging\n",
        "y_pred_counts.rename(columns={'index': 'Value'}, inplace=True)\n",
        "y_test_counts.rename(columns={'index': 'Value'}, inplace=True)\n",
        "\n",
        "# Merge the counts and align them by 'Value'\n",
        "merged_counts = pd.merge(y_pred_counts, y_test_counts, on='Value', how='outer')\n",
        "\n",
        "# Display the result\n",
        "merged_counts.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UaasiIYCv6wX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs('trained_model', exist_ok=True)\n",
        "model.save(os.path.join('trained_model', 'DL_verB2_trained.keras'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHmQLHu_QwI9"
      },
      "source": [
        "## Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUQ7HWCzLApD",
        "outputId": "383e61f4-ab67-4ee5-9bc0-94488aa5755a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing Fold 1...\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step\n",
            "--- Fold 1 Results ---\n",
            "Accuracy :  0.9459459459459459\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.97        35\n",
            "           1       0.00      0.00      0.00         2\n",
            "\n",
            "    accuracy                           0.95        37\n",
            "   macro avg       0.47      0.50      0.49        37\n",
            "weighted avg       0.89      0.95      0.92        37\n",
            "\n",
            "--------------------\n",
            "Processing Fold 2...\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002337813FE20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 994ms/stepWARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002337813FE20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 959ms/step\n",
            "--- Fold 2 Results ---\n",
            "Accuracy :  0.8648648648648649\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      1.00      0.93        32\n",
            "           1       0.00      0.00      0.00         5\n",
            "\n",
            "    accuracy                           0.86        37\n",
            "   macro avg       0.43      0.50      0.46        37\n",
            "weighted avg       0.75      0.86      0.80        37\n",
            "\n",
            "--------------------\n",
            "Processing Fold 3...\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step\n",
            "--- Fold 3 Results ---\n",
            "Accuracy :  0.9459459459459459\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.97        35\n",
            "           1       0.00      0.00      0.00         2\n",
            "\n",
            "    accuracy                           0.95        37\n",
            "   macro avg       0.47      0.50      0.49        37\n",
            "weighted avg       0.89      0.95      0.92        37\n",
            "\n",
            "--------------------\n",
            "Processing Fold 4...\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step\n",
            "--- Fold 4 Results ---\n",
            "Accuracy :  0.8648648648648649\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      1.00      0.93        32\n",
            "           1       0.00      0.00      0.00         5\n",
            "\n",
            "    accuracy                           0.86        37\n",
            "   macro avg       0.43      0.50      0.46        37\n",
            "weighted avg       0.75      0.86      0.80        37\n",
            "\n",
            "--------------------\n",
            "Processing Fold 5...\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step\n",
            "--- Fold 5 Results ---\n",
            "Accuracy :  0.918918918918919\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      1.00      0.96        34\n",
            "           1       0.00      0.00      0.00         3\n",
            "\n",
            "    accuracy                           0.92        37\n",
            "   macro avg       0.46      0.50      0.48        37\n",
            "weighted avg       0.84      0.92      0.88        37\n",
            "\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits=5) # You can adjust the number of splits\n",
        "\n",
        "fold_accuracies = []\n",
        "fold_classification_reports = []\n",
        "fold = 0\n",
        "for train_index, test_index in tscv.split(X_seq):\n",
        "    fold += 1\n",
        "    print(f\"Processing Fold {fold}...\")\n",
        "\n",
        "    # Split data into train and test for this fold\n",
        "    X_train_fold, X_test_fold = X_seq[train_index], X_seq[test_index]\n",
        "    y_train_fold, y_test_fold = Y_seq[train_index], Y_seq[test_index]\n",
        "\n",
        "    # Further split the training fold into train and validation\n",
        "    # We maintain the time series nature, so no shuffling\n",
        "    # Split size for validation can be adjusted\n",
        "    val_size = int(len(X_train_fold) * 0.1)\n",
        "    X_train_inner, X_val_fold = X_train_fold[:-val_size], X_train_fold[-val_size:]\n",
        "    y_train_inner, y_val_fold = y_train_fold[:-val_size], y_train_fold[-val_size:]\n",
        "\n",
        "\n",
        "    # Reshape for scaling\n",
        "    X_train_inner_reshaped = X_train_inner.reshape(-1, X_train_inner.shape[-1])\n",
        "    X_val_fold_reshaped = X_val_fold.reshape(-1, X_val_fold.shape[-1])\n",
        "    X_test_fold_reshaped = X_test_fold.reshape(-1, X_test_fold.shape[-1])\n",
        "\n",
        "    # Initialize and fit scaler on the training data of this fold\n",
        "    scaler_fold = MinMaxScaler()\n",
        "    X_train_inner_scaled = scaler_fold.fit_transform(X_train_inner_reshaped)\n",
        "\n",
        "    # Transform validation and test data using the scaler fitted on the training data\n",
        "    X_val_fold_scaled = scaler_fold.transform(X_val_fold_reshaped)\n",
        "    X_test_fold_scaled = scaler_fold.transform(X_test_fold_reshaped)\n",
        "\n",
        "    # Reshape back to original 3D shape\n",
        "    X_train_inner_seq = X_train_inner_scaled.reshape(X_train_inner.shape)\n",
        "    X_val_fold_seq = X_val_fold_scaled.reshape(X_val_fold.shape)\n",
        "    X_test_fold_seq = X_test_fold_scaled.reshape(X_test_fold.shape)\n",
        "\n",
        "\n",
        "    # Build and compile a fresh model for each fold\n",
        "    model_fold = build_lstm_model()\n",
        "\n",
        "    # Define callbacks\n",
        "    lr_scheduler_fold = ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        min_lr=1e-5,\n",
        "        verbose=0 # Set verbose to 0 for cleaner output\n",
        "    )\n",
        "    early_stopping_fold = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    history_fold = model_fold.fit(\n",
        "        X_train_inner_seq, y_train_inner,\n",
        "        validation_data=(X_val_fold_seq, y_val_fold),\n",
        "        epochs=100,\n",
        "        batch_size=64,\n",
        "        callbacks=[early_stopping_fold, lr_scheduler_fold],\n",
        "        verbose=0 # Set verbose to 0 for cleaner output\n",
        "    )\n",
        "\n",
        "    # Evaluate the model on the test set of this fold\n",
        "    y_pred_fold = model_fold.predict(X_test_fold_seq)\n",
        "    y_pred_fold = (y_pred_fold >= 0.5).astype(int)\n",
        "\n",
        "    # Store accuracy & classification Report\n",
        "    accuracy = accuracy_score(y_test_fold, y_pred_fold)\n",
        "    fold_accuracies.append(accuracy)\n",
        "    report_dict = classification_report(y_test_fold, y_pred_fold, zero_division=0, output_dict=True)\n",
        "    fold_classification_reports.append(report_dict)\n",
        "\n",
        "    # Print evaluation metrics for the fold\n",
        "    print(f\"--- Fold {fold} Results ---\")\n",
        "    print(\"Accuracy : \", accuracy)\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test_fold, y_pred_fold, zero_division=0))\n",
        "    print(\"-\" * 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ns-UYq1UBfd",
        "outputId": "c55dc1ca-92d2-44a6-c963-5d7bb58659b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Cross-Validation Summary ---\n",
            "Average Accuracy  : 0.9081\n",
            "Standard Deviation: 0.0367\n",
            "\n",
            "Average Classification Report:\n",
            "              precision    recall  f1-score    support\n",
            "0              0.908108  1.000000  0.951453  33.600000\n",
            "1              0.000000  0.000000  0.000000   3.400000\n",
            "accuracy       0.908108  0.908108  0.908108   0.908108\n",
            "macro avg      0.454054  0.500000  0.475726  37.000000\n",
            "weighted avg   0.826004  0.908108  0.864764  37.000000\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Cross-Validation Summary ---\")\n",
        "print(f\"Average Accuracy  : {np.mean(fold_accuracies):.4f}\")\n",
        "print(f\"Standard Deviation: {np.std(fold_accuracies):.4f}\")\n",
        "\n",
        "# Convert each report dict into a DataFrame\n",
        "reports_dfs = [pd.DataFrame(report).transpose() for report in fold_classification_reports]\n",
        "# Concatenate all DataFrames into one with a MultiIndex (fold index, class label)\n",
        "all_reports_df = pd.concat(reports_dfs, keys=range(len(reports_dfs)), axis=0)\n",
        "# Group by class label (level=1) and compute mean of metrics\n",
        "mean_report_df = all_reports_df.groupby(level=1).mean()\n",
        "\n",
        "print(\"\\nAverage Classification Report:\")\n",
        "print(mean_report_df)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
